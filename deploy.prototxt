######
#TRAIN INPUT LAYER
#
layer {
    name: "data"
    type: "Input"
    top: "data"
    input_param { shape: { dim: 1 dim: 3 dim: 416 dim: 416 } } 
}

################################################

############### L A Y E R  0 ###################

################## C O N V #####################

layer{
	name: "conv0"
	type: "Convolution"
	bottom: "data"
	top: "conv0"	
	param {
		lr_mult:1  #weights
	}
	param {
		lr_mult:2  #biases
	}
	convolution_param {
		num_output: 32 
		pad: 1
		kernel_size: 3 
		weight_filler {
			type: "xavier" 
		}
		bias_filler {
			type: "constant" 
			value: 0 
		}
	}
}

################## R E L U #####################

layer {
	name: "relu0"
	type: "ReLU"
	bottom: "conv0"
	top: "conv0"
	relu_param{
		negative_slope: 1
	}
}

########## BATCH NORMALISATION LAYER ###########

layer{
	name: "bn0"
	type: "BatchNorm"
	bottom: "conv0"
	top: "bn0"
}


################################################

############### L A Y E R  1 ###################

################## C O N V #####################


layer{
	name: "conv1"
	type: "Convolution"
	bottom: "bn0"
	top: "conv1"	
	param {
		lr_mult:1  #weights
	}
	param {
		lr_mult:2  #biases
	}
	convolution_param {
		num_output: 64 
		pad: 1
		kernel_size: 3
		stride: 2 
		weight_filler {
			type: "xavier" 
		}
		bias_filler {
			type: "constant" 
			value: 0 
		}
	}
}

################## R E L U #####################

layer {
	name: "relu1"
	type: "ReLU"
	bottom: "conv1"
	top: "conv1"
	relu_param{
                negative_slope: 1
        }
}

########## BATCH NORMALISATION LAYER ###########

layer{
        name: "bn1"
        type: "BatchNorm"
        bottom: "conv1"
        top: "bn1"
}

################################################

############### L A Y E R  2 ###################

################## C O N V #####################

layer{
	name: "conv2"
	type: "Convolution"
	bottom: "bn1"
	top: "conv2"	
	param {
		lr_mult:1  #weights
	}
	param {
		lr_mult:2  #biases
	}
	convolution_param {
		num_output: 32 
		pad: 1
		kernel_size: 1 
		weight_filler {
			type: "xavier" 
		}
		bias_filler {
			type: "constant" 
			value: 0 
		}
	}
}

################## R E L U #####################

layer {
	name: "relu2"
	type: "ReLU"
	bottom: "conv2"
	top: "conv2"
	relu_param{
                negative_slope: 1
        }
}

########## BATCH NORMALISATION LAYER ###########

layer{
        name: "bn2"
        type: "BatchNorm"
        bottom: "conv2"
        top: "bn2"
}


################################################

############### L A Y E R  3 ###################

################## C O N V #####################

layer{
	name: "conv3"
	type: "Convolution"
	bottom: "bn2"
	top: "conv3"	
	param {
		lr_mult:1  #weights
	}
	param {
		lr_mult:2  #biases
	}
	convolution_param {
		num_output: 64 
		#pad: 1
		kernel_size: 3 
		weight_filler {
			type: "xavier" 
		}
		bias_filler {
			type: "constant" 
			value: 0 
		}
	}
}

################## R E L U #####################

layer {
	name: "relu3"
	type: "ReLU"
	bottom: "conv3"
	top: "conv3"
	relu_param{
                negative_slope: 1
        }
}

########## BATCH NORMALISATION LAYER ###########

layer{
        name: "bn3"
        type: "BatchNorm"
        bottom: "conv3"
        top: "bn3"
}


################################################

############### L A Y E R  4 ###################
        
############## R E S I D U A L #################

layer {
	bottom: "bn3"
	bottom: "bn1"
	top: "res4"
	name: "res4"
	type: "Eltwise"
	eltwise_param{
		operation: SUM
	}
}

################## R E L U #####################

layer {
        name: "relu4"
        type: "ReLU"
        bottom: "res4"
        top: "res4"
}


################################################

############### L A Y E R  5 ###################

################## C O N V #####################

layer{
	name: "conv5"
	type: "Convolution"
	bottom: "res4"
	top: "conv5"	
	param {
		lr_mult:1  #weights
	}
	param {
		lr_mult:2  #biases
	}
	convolution_param {
		num_output: 128
		pad: 1
		kernel_size: 3
		stride: 2 
		weight_filler {
			type: "xavier" 
		}
		bias_filler {
			type: "constant" 
			value: 0 
		}
	}
}

################## R E L U #####################

layer {
	name: "relu5"
	type: "ReLU"
	bottom: "conv5"
	top: "conv5"
	relu_param{
                negative_slope: 1
        }
}

########## BATCH NORMALISATION LAYER ###########

layer{
        name: "bn5"
        type: "BatchNorm"
        bottom: "conv5"
        top: "bn5"
}


################################################

############### L A Y E R  6 ###################

################## C O N V #####################


layer{
	name: "conv6"
	type: "Convolution"
	bottom: "bn5"
	top: "conv6"	
	param {
		lr_mult:1  #weights
	}
	param {
		lr_mult:2  #biases
	}
	convolution_param {
		num_output: 64 
		pad: 1
		kernel_size: 1 
		weight_filler {
			type: "xavier" 
		}
		bias_filler {
			type: "constant" 
			value: 0 
		}
	}
}

################## R E L U #####################

layer {
	name: "relu6"
	type: "ReLU"
	bottom: "conv6"
	top: "conv6"
	relu_param{
                negative_slope: 1
        }
}

########## BATCH NORMALISATION LAYER ###########

layer{
        name: "bn6"
        type: "BatchNorm"
        bottom: "conv6"
        top: "bn6"
}


################################################

############### L A Y E R  7 ###################

################## C O N V #####################

layer{
	name: "conv7"
	type: "Convolution"
	bottom: "bn6"
	top: "conv7"	
	param {
		lr_mult:1  #weights
	}
	param {
		lr_mult:2  #biases
	}
	convolution_param {
		num_output: 128 
		#pad: 1
		kernel_size: 3 
		weight_filler {
			type: "xavier" 
		}
		bias_filler {
			type: "constant" 
			value: 0 
		}
	}
}

################## R E L U #####################

layer {
	name: "relu7"
	type: "ReLU"
	bottom: "conv7"
	top: "conv7"
	relu_param{
                negative_slope: 1
        }
}

########## BATCH NORMALISATION LAYER ###########

layer{
        name: "bn7"
        type: "BatchNorm"
        bottom: "conv7"
        top: "bn7"
}


################################################

############### L A Y E R  8 ###################

############## R E S I D U A L #################

layer {
	bottom: "bn5"
	bottom: "bn7"
	top: "res8"
	name: "res8"
	type: "Eltwise"
	eltwise_param{
		operation: SUM
	}
}

################## R E L U #####################

layer {
        name: "relu8"
        type: "ReLU"
        bottom: "res8"
        top: "res8"
}


################################################

############### L A Y E R  9 ###################

################## C O N V #####################


layer{
	name: "conv9"
	type: "Convolution"
	bottom: "res8"
	top: "conv9"	
	param {
		lr_mult:1  #weights
	}
	param {
		lr_mult:2  #biases
	}
	convolution_param {
		num_output: 64 
		pad: 1
		kernel_size: 1 
		weight_filler {
			type: "xavier" 
		}
		bias_filler {
			type: "constant" 
			value: 0 
		}
	}
}

################## R E L U #####################

layer {
	name: "relu9"
	type: "ReLU"
	bottom: "conv9"
	top: "conv9"
	relu_param{
                negative_slope: 1
        }
}

########## BATCH NORMALISATION LAYER ###########

layer{
        name: "bn9"
        type: "BatchNorm"
        bottom: "conv9"
        top: "bn9"
}


################################################

############### L A Y E R  10 ###################

################## C O N V #####################

layer{
	name: "conv10"
	type: "Convolution"
	bottom: "bn9"
	top: "conv10"	
	param {
		lr_mult:1  #weights
	}
	param {
		lr_mult:2  #biases
	}
	convolution_param {
		num_output: 128 
		#pad: 1
		kernel_size: 3 
		weight_filler {
			type: "xavier" 
		}
		bias_filler {
			type: "constant" 
			value: 0 
		}
	}
}

################## R E L U #####################

layer {
	name: "relu10"
	type: "ReLU"
	bottom: "conv10"
	top: "conv10"
	relu_param{
                negative_slope: 1
        }
}

########## BATCH NORMALISATION LAYER ###########

layer{
        name: "bn10"
        type: "BatchNorm"
        bottom: "conv10"
        top: "bn10"
}


################################################

############### L A Y E R  11 ###################

############## R E S I D U A L #################

layer {
	bottom: "bn10"
	bottom: "res8"
	top: "res11"
	name: "res11"
	type: "Eltwise"
	eltwise_param{
		operation: SUM
	}
}

################## R E L U #####################

layer {
        name: "relu11"
        type: "ReLU"
        bottom: "res11"
        top: "res11"
}


################################################

############### L A Y E R  12 ###################

################## C O N V #####################

layer{
	name: "conv12"
	type: "Convolution"
	bottom: "res11"
	top: "conv12"	
	param {
		lr_mult:1  #weights
	}
	param {
		lr_mult:2  #biases
	}
	convolution_param {
		num_output: 256
		pad: 1
		kernel_size: 3
		stride: 2 
		weight_filler {
			type: "xavier" 
		}
		bias_filler {
			type: "constant" 
			value: 0 
		}
	}
}

################## R E L U #####################

layer {
	name: "relu12"
	type: "ReLU"
	bottom: "conv12"
	top: "conv12"
	relu_param{
                negative_slope: 1
        }
}

########## BATCH NORMALISATION LAYER ###########

layer{
        name: "bn12"
        type: "BatchNorm"
        bottom: "conv12"
        top: "bn12"
}


################################################

############### L A Y E R  13 ###################

################## C O N V #####################

layer{
	name: "conv13"
	type: "Convolution"
	bottom: "bn12"
	top: "conv13"	
	param {
		lr_mult:1  #weights
	}
	param {
		lr_mult:2  #biases
	}
	convolution_param {
		num_output: 128 
		pad: 1
		kernel_size: 1 
		weight_filler {
			type: "xavier" 
		}
		bias_filler {
			type: "constant" 
			value: 0 
		}
	}
}

################## R E L U #####################

layer {
	name: "relu13"
	type: "ReLU"
	bottom: "conv13"
	top: "conv13"
	relu_param{
                negative_slope: 1
        }
}

########## BATCH NORMALISATION LAYER ###########

layer{
        name: "bn13"
        type: "BatchNorm"
        bottom: "conv13"
        top: "bn13"
}


################################################

############### L A Y E R  14 ###################

################## C O N V #####################

layer{
	name: "conv14"
	type: "Convolution"
	bottom: "bn13"
	top: "conv14"	
	param {
		lr_mult:1  #weights
	}
	param {
		lr_mult:2  #biases
	}
	convolution_param {
		num_output: 256 
		#pad: 1
		kernel_size: 3 
		weight_filler {
			type: "xavier" 
		}
		bias_filler {
			type: "constant" 
			value: 0 
		}
	}
}

################## R E L U #####################

layer {
	name: "relu14"
	type: "ReLU"
	bottom: "conv14"
	top: "conv14"
	relu_param{
                negative_slope: 1
        }
}

########## BATCH NORMALISATION LAYER ###########

layer{
        name: "bn14"
        type: "BatchNorm"
        bottom: "conv14"
        top: "bn14"
}


################################################

############### L A Y E R  15 ###################

############## R E S I D U A L #################

layer {
	bottom: "bn14"
	bottom: "bn12"
	top: "res15"
	name: "res15"
	type: "Eltwise"
	eltwise_param{
		operation: SUM
	}
}

################## R E L U #####################

layer {
        name: "relu15"
        type: "ReLU"
        bottom: "res15"
        top: "res15"
}


################################################

############### L A Y E R  16 ###################

################## C O N V #####################

layer{
	name: "conv16"
	type: "Convolution"
	bottom: "res15"
	top: "conv16"	
	param {
		lr_mult:1  #weights
	}
	param {
		lr_mult:2  #biases
	}
	convolution_param {
		num_output: 128 
		pad: 1
		kernel_size: 1 
		weight_filler {
			type: "xavier" 
		}
		bias_filler {
			type: "constant" 
			value: 0 
		}
	}
}

################## R E L U #####################

layer {
	name: "relu16"
	type: "ReLU"
	bottom: "conv16"
	top: "conv16"
	relu_param{
                negative_slope: 1
        }
}

########## BATCH NORMALISATION LAYER ###########

layer{
        name: "bn16"
        type: "BatchNorm"
        bottom: "conv16"
        top: "bn16"
}


################################################

############### L A Y E R  17 ###################

################## C O N V #####################


layer{
	name: "conv17"
	type: "Convolution"
	bottom: "bn16"
	top: "conv17"	
	param {
		lr_mult:1  #weights
	}
	param {
		lr_mult:2  #biases
	}
	convolution_param {
		num_output: 256 
		#pad: 1
		kernel_size: 3 
		weight_filler {
			type: "xavier" 
		}
		bias_filler {
			type: "constant" 
			value: 0 
		}
	}
}

################## R E L U #####################

layer {
	name: "relu17"
	type: "ReLU"
	bottom: "conv17"
	top: "conv17"
	relu_param{
                negative_slope: 1
        }
}

########## BATCH NORMALISATION LAYER ###########

layer{
        name: "bn17"
        type: "BatchNorm"
        bottom: "conv17"
        top: "bn17"
}


################################################

############### L A Y E R  18 ###################

############## R E S I D U A L #################

layer {
	bottom: "bn17"
	bottom: "res15"
	top: "res18"
	name: "res18"
	type: "Eltwise"
	eltwise_param{
		operation: SUM
	}
}

################## R E L U #####################

layer {
        name: "relu18"
        type: "ReLU"
        bottom: "res18"
        top: "res18"
}

################################################

############### L A Y E R  19 ###################

################## C O N V #####################

layer{
	name: "conv19"
	type: "Convolution"
	bottom: "res18"
	top: "conv19"	
	param {
		lr_mult:1  #weights
	}
	param {
		lr_mult:2  #biases
	}
	convolution_param {
		num_output: 128 
		pad: 1
		kernel_size: 1 
		weight_filler {
			type: "xavier" 
		}
		bias_filler {
			type: "constant" 
			value: 0 
		}
	}
}

################## R E L U #####################

layer {
	name: "relu19"
	type: "ReLU"
	bottom: "conv19"
	top: "conv19"
	relu_param{
                negative_slope: 1
        }
}

########## BATCH NORMALISATION LAYER ###########

layer{
        name: "bn19"
        type: "BatchNorm"
        bottom: "conv19"
        top: "bn19"
}



################################################

############### L A Y E R  20 ###################

################## C O N V #####################

layer{
	name: "conv20"
	type: "Convolution"
	bottom: "bn19"
	top: "conv20"	
	param {
		lr_mult:1  #weights
	}
	param {
		lr_mult:2  #biases
	}
	convolution_param {
		num_output: 256 
		#pad: 1
		kernel_size: 3 
		weight_filler {
			type: "xavier" 
		}
		bias_filler {
			type: "constant" 
			value: 0 
		}
	}
}

################## R E L U #####################

layer {
	name: "relu20"
	type: "ReLU"
	bottom: "conv20"
	top: "conv20"
	relu_param{
                negative_slope: 1
        }
}

########## BATCH NORMALISATION LAYER ###########

layer{
        name: "bn20"
        type: "BatchNorm"
        bottom: "conv20"
        top: "bn20"
}


################################################

############### L A Y E R  21 ###################

############## R E S I D U A L #################

layer {
	bottom: "res18"
	bottom: "bn20"
	top: "res21"
	name: "res21"
	type: "Eltwise"
	eltwise_param{
		operation: SUM
	}
}

################## R E L U #####################

layer {
        name: "relu21"
        type: "ReLU"
        bottom: "res21"
        top: "res21"
}


################################################

############### L A Y E R  22 ###################

################## C O N V #####################

layer{
	name: "conv22"
	type: "Convolution"
	bottom: "res21"
	top: "conv22"	
	param {
		lr_mult:1  #weights
	}
	param {
		lr_mult:2  #biases
	}
	convolution_param {
		num_output: 128 
		pad: 1
		kernel_size: 1 
		weight_filler {
			type: "xavier" 
		}
		bias_filler {
			type: "constant" 
			value: 0 
		}
	}
}

################## R E L U #####################

layer {
	name: "relu22"
	type: "ReLU"
	bottom: "conv22"
	top: "conv22"
	relu_param{
                negative_slope: 1
        }
}

########## BATCH NORMALISATION LAYER ###########

layer{
        name: "bn22"
        type: "BatchNorm"
        bottom: "conv22"
        top: "bn22"
}



################################################

############### L A Y E R  23 ###################

################## C O N V #####################

layer{
	name: "conv23"
	type: "Convolution"
	bottom: "bn22"
	top: "conv23"	
	param {
		lr_mult:1  #weights
	}
	param {
		lr_mult:2  #biases
	}
	convolution_param {
		num_output: 256 
		#pad: 1
		kernel_size: 3 
		weight_filler {
			type: "xavier" 
		}
		bias_filler {
			type: "constant" 
			value: 0 
		}
	}
}

################## R E L U #####################

layer {
	name: "relu23"
	type: "ReLU"
	bottom: "conv23"
	top: "conv23"
	relu_param{
                negative_slope: 1
        }
}

# BATCH NORMALISATION LAYER

layer{
        name: "bn23"
        type: "BatchNorm"
        bottom: "conv23"
        top: "bn23"
}


################################################

############### L A Y E R  24 ###################

############## R E S I D U A L #################

layer {
	bottom: "res21"
	bottom: "bn23"
	top: "res24"
	name: "res24"
	type: "Eltwise"
	eltwise_param{
		operation: SUM
	}
}

################## R E L U #####################

layer {
        name: "relu24"
        type: "ReLU"
        bottom: "res24"
        top: "res24"
}



################################################

############### L A Y E R  25 ###################

################## C O N V #####################

layer{
	name: "conv25"
	type: "Convolution"
	bottom: "res24"
	top: "conv25"	
	param {
		lr_mult:1  #weights
	}
	param {
		lr_mult:2  #biases
	}
	convolution_param {
		num_output: 128 
		pad: 1
		kernel_size: 1 
		weight_filler {
			type: "xavier" 
		}
		bias_filler {
			type: "constant" 
			value: 0 
		}
	}
}

################## R E L U #####################

layer {
	name: "relu25"
	type: "ReLU"
	bottom: "conv25"
	top: "conv25"
	relu_param{
                negative_slope: 1
        }
}

########## BATCH NORMALISATION LAYER ###########

layer{
        name: "bn25"
        type: "BatchNorm"
        bottom: "conv25"
        top: "bn25"
}



################################################

############### L A Y E R  26 ###################

################## C O N V #####################

layer{
	name: "conv26"
	type: "Convolution"
	bottom: "bn25"
	top: "conv26"	
	param {
		lr_mult:1  #weights
	}
	param {
		lr_mult:2  #biases
	}
	convolution_param {
		num_output: 256 
		#pad: 1
		kernel_size: 3 
		weight_filler {
			type: "xavier" 
		}
		bias_filler {
			type: "constant" 
			value: 0 
		}
	}
}

################## R E L U #####################

layer {
	name: "relu26"
	type: "ReLU"
	bottom: "conv26"
	top: "conv26"
	relu_param{
                negative_slope: 1
        }
}

########## BATCH NORMALISATION LAYER ###########

layer{
        name: "bn26"
        type: "BatchNorm"
        bottom: "conv26"
        top: "bn26"
}


################################################

############### L A Y E R  27 ###################

############## R E S I D U A L #################

layer {
	bottom: "bn26"
	bottom: "res24"
	top: "res27"
	name: "res27"
	type: "Eltwise"
	eltwise_param{
		operation: SUM
	}
}

################## R E L U #####################

layer {
        name: "relu27"
        type: "ReLU"
        bottom: "res27"
        top: "res27"
}



################################################

############### L A Y E R  28 ###################

################## C O N V #####################

layer{
	name: "conv28"
	type: "Convolution"
	bottom: "res27"
	top: "conv28"	
	param {
		lr_mult:1  #weights
	}
	param {
		lr_mult:2  #biases
	}
	convolution_param {
		num_output: 128 
		pad: 1
		kernel_size: 1 
		weight_filler {
			type: "xavier" 
		}
		bias_filler {
			type: "constant" 
			value: 0 
		}
	}
}

################## R E L U #####################

layer {
	name: "relu28"
	type: "ReLU"
	bottom: "conv28"
	top: "conv28"
	relu_param{
                negative_slope: 1
        }
}

########## BATCH NORMALISATION LAYER ###########

layer{
        name: "bn28"
        type: "BatchNorm"
        bottom: "conv28"
        top: "bn28"
}



################################################

############### L A Y E R  29 ###################

################## C O N V #####################

layer{
	name: "conv29"
	type: "Convolution"
	bottom: "bn28"
	top: "conv29"	
	param {
		lr_mult:1  #weights
	}
	param {
		lr_mult:2  #biases
	}
	convolution_param {
		num_output: 256 
		#pad: 1
		kernel_size: 3 
		weight_filler {
			type: "xavier" 
		}
		bias_filler {
			type: "constant" 
			value: 0 
		}
	}
}

################## R E L U #####################

layer {
	name: "relu29"
	type: "ReLU"
	bottom: "conv29"
	top: "conv29"
	relu_param{
                negative_slope: 1
        }
}

########## BATCH NORMALISATION LAYER ###########

layer{
        name: "bn29"
        type: "BatchNorm"
        bottom: "conv29"
        top: "bn29"
}


################################################

############### L A Y E R  30 ###################

############## R E S I D U A L #################

layer {
	bottom: "res27"
	bottom: "bn29"
	top: "res30"
	name: "res30"
	type: "Eltwise"
	eltwise_param{
		operation: SUM
	}
}

################## R E L U #####################

layer {
        name: "relu30"
        type: "ReLU"
        bottom: "res30"
        top: "res30"
}

################################################

############### L A Y E R  31 ###################

################## C O N V #####################

layer{
	name: "conv31"
	type: "Convolution"
	bottom: "res30"
	top: "conv31"	
	param {
		lr_mult:1  #weights
	}
	param {
		lr_mult:2  #biases
	}
	convolution_param {
		num_output: 128 
		pad: 1
		kernel_size: 1 
		weight_filler {
			type: "xavier" 
		}
		bias_filler {
			type: "constant" 
			value: 0 
		}
	}
}

################## R E L U #####################

layer {
	name: "relu31"
	type: "ReLU"
	bottom: "conv31"
	top: "conv31"
	relu_param{
                negative_slope: 1
        }
}

########## BATCH NORMALISATION LAYER ###########

layer{
        name: "bn31"
        type: "BatchNorm"
        bottom: "conv31"
        top: "bn31"
}




################################################

############### L A Y E R  32 ###################

################## C O N V #####################

layer{
	name: "conv32"
	type: "Convolution"
	bottom: "bn31"
	top: "conv32"	
	param {
		lr_mult:1  #weights
	}
	param {
		lr_mult:2  #biases
	}
	convolution_param {
		num_output: 256 
		#pad: 1
		kernel_size: 3 
		weight_filler {
			type: "xavier" 
		}
		bias_filler {
			type: "constant" 
			value: 0 
		}
	}
}

################## R E L U #####################

layer {
	name: "relu32"
	type: "ReLU"
	bottom: "conv32"
	top: "conv32"
	relu_param{
                negative_slope: 1
        }
}

########## BATCH NORMALISATION LAYER ###########

layer{
        name: "bn32"
        type: "BatchNorm"
        bottom: "conv32"
        top: "bn32"
}


################################################

############### L A Y E R  33 ###################

############## R E S I D U A L #################

layer {
	bottom: "bn32"
	bottom: "res30"
	top: "res33"
	name: "res3"
	type: "Eltwise"
	eltwise_param{
		operation: SUM
	}
}

################## R E L U #####################

layer {
        name: "relu33"
        type: "ReLU"
        bottom: "res33"
        top: "res33"
}


################################################

############### L A Y E R  34 ###################

################## C O N V #####################

layer{
	name: "conv34"
	type: "Convolution"
	bottom: "res33"
	top: "conv34"	
	param {
		lr_mult:1  #weights
	}
	param {
		lr_mult:2  #biases
	}
	convolution_param {
		num_output: 128 
		pad: 1
		kernel_size: 1 
		weight_filler {
			type: "xavier" 
		}
		bias_filler {
			type: "constant" 
			value: 0 
		}
	}
}

################## R E L U #####################

layer {
	name: "relu34"
	type: "ReLU"
	bottom: "conv34"
	top: "conv34"
	relu_param{
                negative_slope: 1
        }
}

########## BATCH NORMALISATION LAYER ###########

layer{
        name: "bn34"
        type: "BatchNorm"
        bottom: "conv34"
        top: "bn34"
}



################################################

############### L A Y E R  35 ###################

################## C O N V #####################

layer{
	name: "conv35"
	type: "Convolution"
	bottom: "bn34"
	top: "conv35"	
	param {
		lr_mult:1  #weights
	}
	param {
		lr_mult:2  #biases
	}
	convolution_param {
		num_output: 256 
		#pad: 1
		kernel_size: 3 
		weight_filler {
			type: "xavier" 
		}
		bias_filler {
			type: "constant" 
			value: 0 
		}
	}
}

################## R E L U #####################

layer {
	name: "relu35"
	type: "ReLU"
	bottom: "conv35"
	top: "conv35"
	relu_param{
                negative_slope: 1
        }
}

########## BATCH NORMALISATION LAYER ###########

layer{
        name: "bn35"
        type: "BatchNorm"
        bottom: "conv35"
        top: "bn35"
}


################################################

############### L A Y E R  36 ###################

############## R E S I D U A L #################

layer {
	bottom: "bn35"
	bottom: "res33"
	top: "res36"
	name: "res36"
	type: "Eltwise"
	eltwise_param{
		operation: SUM
	}
}

################## R E L U #####################

layer {
        name: "relu36"
        type: "ReLU"
        bottom: "res36"
        top: "res36"
}


################################################

############### L A Y E R  37 ###################

################## C O N V #####################

layer{
	name: "conv37"
	type: "Convolution"
	bottom: "res36"
	top: "conv37"	
	param {
		lr_mult:1  #weights
	}
	param {
		lr_mult:2  #biases
	}
	convolution_param {
		num_output: 512
		pad: 1
		kernel_size: 3
		stride: 2 
		weight_filler {
			type: "xavier" 
		}
		bias_filler {
			type: "constant" 
			value: 0 
		}
	}
}

################## R E L U #####################

layer {
	name: "relu37"
	type: "ReLU"
	bottom: "conv37"
	top: "conv37"
	relu_param{
                negative_slope: 1
        }
}

########## BATCH NORMALISATION LAYER ###########

layer{
        name: "bn37"
        type: "BatchNorm"
        bottom: "conv37"
        top: "bn37"
}


################################################

############### L A Y E R  38 ###################

################## C O N V #####################

layer{
	name: "conv38"
	type: "Convolution"
	bottom: "bn37"
	top: "conv38"	
	param {
		lr_mult:1  #weights
	}
	param {
		lr_mult:2  #biases
	}
	convolution_param {
		num_output: 256 
		pad: 1
		kernel_size: 1 
		weight_filler {
			type: "xavier" 
		}
		bias_filler {
			type: "constant" 
			value: 0 
		}
	}
}

################## R E L U #####################

layer {
	name: "relu38"
	type: "ReLU"
	bottom: "conv38"
	top: "conv38"
	relu_param{
                negative_slope: 1
        }
}

########## BATCH NORMALISATION LAYER ###########

layer{
        name: "bn38"
        type: "BatchNorm"
        bottom: "conv38"
        top: "bn38"
}



################################################

############### L A Y E R  39 ###################

################## C O N V #####################

layer{
	name: "conv39"
	type: "Convolution"
	bottom: "bn38"
	top: "conv39"	
	param {
		lr_mult:1  #weights
	}
	param {
		lr_mult:2  #biases
	}
	convolution_param {
		num_output: 512 
		#pad: 1
		kernel_size: 3 
		weight_filler {
			type: "xavier" 
		}
		bias_filler {
			type: "constant" 
			value: 0 
		}
	}
}

################## R E L U #####################

layer {
	name: "relu39"
	type: "ReLU"
	bottom: "conv39"
	top: "conv39"
	relu_param{
                negative_slope: 1
        }
}

########## BATCH NORMALISATION LAYER ###########

layer{
        name: "bn39"
        type: "BatchNorm"
        bottom: "conv39"
        top: "bn39"
}


################################################

############### L A Y E R  40 ###################

############## R E S I D U A L #################

layer {
	bottom: "bn39"
	bottom: "bn37"
	top: "res40"
	name: "res40"
	type: "Eltwise"
	eltwise_param{
		operation: SUM
	}
}

################## R E L U #####################

layer {
        name: "relu40"
        type: "ReLU"
        bottom: "res40"
        top: "res40"
}


################################################

############### L A Y E R  41 ###################

################## C O N V #####################

layer{
	name: "conv41"
	type: "Convolution"
	bottom: "res40"
	top: "conv41"	
	param {
		lr_mult:1  #weights
	}
	param {
		lr_mult:2  #biases
	}
	convolution_param {
		num_output: 256
		pad: 1
		kernel_size: 1 
		weight_filler {
			type: "xavier" 
		}
		bias_filler {
			type: "constant" 
			value: 0 
		}
	}
}

################## R E L U #####################

layer {
	name: "relu41"
	type: "ReLU"
	bottom: "conv41"
	top: "conv41"
	relu_param{
                negative_slope: 1
        }
}

########## BATCH NORMALISATION LAYER ###########

layer{
        name: "bn41"
        type: "BatchNorm"
        bottom: "conv41"
        top: "bn41"
}



################################################

############### L A Y E R  42 ###################

################## C O N V #####################

layer{
	name: "conv42"
	type: "Convolution"
	bottom: "bn41"
	top: "conv42"	
	param {
		lr_mult:1  #weights
	}
	param {
		lr_mult:2  #biases
	}
	convolution_param {
		num_output: 512 
		#pad: 1
		kernel_size: 3 
		weight_filler {
			type: "xavier" 
		}
		bias_filler {
			type: "constant" 
			value: 0 
		}
	}
}

################## R E L U #####################

layer {
	name: "relu42"
	type: "ReLU"
	bottom: "conv42"
	top: "conv42"
	relu_param{
                negative_slope: 1
        }
}

########## BATCH NORMALISATION LAYER ###########

layer{
        name: "bn42"
        type: "BatchNorm"
        bottom: "conv42"
        top: "bn42"
}


################################################

############### L A Y E R  43 ###################

############## R E S I D U A L #################

layer {
	bottom: "bn42"
	bottom: "res40"
	top: "res43"
	name: "res43"
	type: "Eltwise"
	eltwise_param{
		operation: SUM
	}
}

################## R E L U #####################

layer {
        name: "relu43"
        type: "ReLU"
        bottom: "res43"
        top: "res43"
}


################################################

############### L A Y E R  44 ###################

################## C O N V #####################

layer{
	name: "conv44"
	type: "Convolution"
	bottom: "res43"
	top: "conv44"	
	param {
		lr_mult:1  #weights
	}
	param {
		lr_mult:2  #biases
	}
	convolution_param {
		num_output: 256 
		pad: 1
		kernel_size: 1 
		weight_filler {
			type: "xavier" 
		}
		bias_filler {
			type: "constant" 
			value: 0 
		}
	}
}

################## R E L U #####################

layer {
	name: "relu44"
	type: "ReLU"
	bottom: "conv44"
	top: "conv44"
	relu_param{
                negative_slope: 1
        }
}

########## BATCH NORMALISATION LAYER ###########

layer{
        name: "bn44"
        type: "BatchNorm"
        bottom: "conv44"
        top: "bn44"
}



################################################

############### L A Y E R  45 ###################

################## C O N V #####################

layer{
	name: "conv45"
	type: "Convolution"
	bottom: "bn44"
	top: "conv45"	
	param {
		lr_mult:1  #weights
	}
	param {
		lr_mult:2  #biases
	}
	convolution_param {
		num_output: 512 
		#pad: 1
		kernel_size: 3 
		weight_filler {
			type: "xavier" 
		}
		bias_filler {
			type: "constant" 
			value: 0 
		}
	}
}

################## R E L U #####################

layer {
	name: "relu45"
	type: "ReLU"
	bottom: "conv45"
	top: "conv45"
	relu_param{
                negative_slope: 1
        }
}

########## BATCH NORMALISATION LAYER ###########

layer{
        name: "bn45"
        type: "BatchNorm"
        bottom: "conv45"
        top: "bn45"
}



################################################

############### L A Y E R  46 ###################

############## R E S I D U A L #################

layer {
	bottom: "bn45"
	bottom: "res43"
	top: "res46"
	name: "res46"
	type: "Eltwise"
	eltwise_param{
		operation: SUM
	}
}

################## R E L U #####################

layer {
        name: "relu46"
        type: "ReLU"
        bottom: "res46"
        top: "res46"
}



################################################

############### L A Y E R  47 ###################

################## C O N V #####################

layer{
	name: "conv47"
	type: "Convolution"
	bottom: "res46"
	top: "conv47"	
	param {
		lr_mult:1  #weights
	}
	param {
		lr_mult:2  #biases
	}
	convolution_param {
		num_output: 256 
		pad: 1
		kernel_size: 1 
		weight_filler {
			type: "xavier" 
		}
		bias_filler {
			type: "constant" 
			value: 0 
		}
	}
}

################## R E L U #####################

layer {
	name: "relu47"
	type: "ReLU"
	bottom: "conv47"
	top: "conv47"
	relu_param{
                negative_slope: 1
        }
}

########## BATCH NORMALISATION LAYER ###########

layer{
        name: "bn47"
        type: "BatchNorm"
        bottom: "conv47"
        top: "bn47"
}



################################################

############### L A Y E R  48 ###################

################## C O N V #####################

layer{
	name: "conv48"
	type: "Convolution"
	bottom: "bn47"
	top: "conv48"	
	param {
		lr_mult:1  #weights
	}
	param {
		lr_mult:2  #biases
	}
	convolution_param {
		num_output: 512 
		#pad: 1
		kernel_size: 3 
		weight_filler {
			type: "xavier" 
		}
		bias_filler {
			type: "constant" 
			value: 0 
		}
	}
}

################## R E L U #####################

layer {
	name: "relu48"
	type: "ReLU"
	bottom: "conv48"
	top: "conv48"
	relu_param{
                negative_slope: 1
        }
}

########## BATCH NORMALISATION LAYER ###########

layer{
        name: "bn48"
        type: "BatchNorm"
        bottom: "conv48"
        top: "bn48"
}


################################################

############### L A Y E R  49 ###################

############## R E S I D U A L #################

layer {
	bottom: "bn48"
	bottom: "res46"
	top: "res49"
	name: "res49"
	type: "Eltwise"
	eltwise_param{
		operation: SUM
	}
}

################## R E L U #####################

layer {
        name: "relu49"
        type: "ReLU"
        bottom: "res49"
        top: "res49"
}


################################################

############### L A Y E R  50 ###################

################## C O N V #####################

layer{
	name: "conv50"
	type: "Convolution"
	bottom: "res49"
	top: "conv50"	
	param {
		lr_mult:1  #weights
	}
	param {
		lr_mult:2  #biases
	}
	convolution_param {
		num_output: 256 
		pad: 1
		kernel_size: 1 
		weight_filler {
			type: "xavier" 
		}
		bias_filler {
			type: "constant" 
			value: 0 
		}
	}
}

################## R E L U #####################

layer {
	name: "relu50"
	type: "ReLU"
	bottom: "conv50"
	top: "conv50"
	relu_param{
                negative_slope: 1
        }
}

########## BATCH NORMALISATION LAYER ###########

layer{
        name: "bn50"
        type: "BatchNorm"
        bottom: "conv50"
        top: "bn50"
}



################################################

############### L A Y E R  51 ###################

################## C O N V #####################

layer{
	name: "conv51"
	type: "Convolution"
	bottom: "bn50"
	top: "conv51"	
	param {
		lr_mult:1  #weights
	}
	param {
		lr_mult:2  #biases
	}
	convolution_param {
		num_output: 512 
		#pad: 1
		kernel_size: 3 
		weight_filler {
			type: "xavier" 
		}
		bias_filler {
			type: "constant" 
			value: 0 
		}
	}
}

################## R E L U #####################

layer {
	name: "relu51"
	type: "ReLU"
	bottom: "conv51"
	top: "conv51"
	relu_param{
                negative_slope: 1
        }
}

########## BATCH NORMALISATION LAYER ###########

layer{
        name: "bn51"
        type: "BatchNorm"
        bottom: "conv51"
        top: "bn51"
}


################################################

############### L A Y E R  52 ###################

############## R E S I D U A L #################

layer {
	bottom: "bn51"
	bottom: "res49"
	top: "res52"
	name: "res52"
	type: "Eltwise"
	eltwise_param{
		operation: SUM
	}
}

################## R E L U #####################

layer {
        name: "relu52"
        type: "ReLU"
        bottom: "res52"
        top: "res52"
}



################################################

############### L A Y E R  53 ###################

################## C O N V #####################

layer{
	name: "conv53"
	type: "Convolution"
	bottom: "res52"
	top: "conv53"	
	param {
		lr_mult:1  #weights
	}
	param {
		lr_mult:2  #biases
	}
	convolution_param {
		num_output: 256 
		pad: 1
		kernel_size: 1 
		weight_filler {
			type: "xavier" 
		}
		bias_filler {
			type: "constant" 
			value: 0 
		}
	}
}

################## R E L U #####################

layer {
	name: "relu53"
	type: "ReLU"
	bottom: "conv53"
	top: "conv53"
	relu_param{
                negative_slope: 1
        }
}

########## BATCH NORMALISATION LAYER ###########

layer{
        name: "bn53"
        type: "BatchNorm"
        bottom: "conv53"
        top: "bn53"
}



################################################

############### L A Y E R  54 ###################

################## C O N V #####################

layer{
	name: "conv54"
	type: "Convolution"
	bottom: "bn53"
	top: "conv54"	
	param {
		lr_mult:1  #weights
	}
	param {
		lr_mult:2  #biases
	}
	convolution_param {
		num_output: 512 
		#pad: 1
		kernel_size: 3 
		weight_filler {
			type: "xavier" 
		}
		bias_filler {
			type: "constant" 
			value: 0 
		}
	}
}

################## R E L U #####################

layer {
	name: "relu54"
	type: "ReLU"
	bottom: "conv54"
	top: "conv54"
	relu_param{
                negative_slope: 1
        }
}

########## BATCH NORMALISATION LAYER ###########

layer{
        name: "bn54"
        type: "BatchNorm"
        bottom: "conv54"
        top: "bn54"
}


################################################

############### L A Y E R  55 ###################

############## R E S I D U A L #################

layer {
	bottom: "bn54"
	bottom: "res52"
	top: "res55"
	name: "res55"
	type: "Eltwise"
	eltwise_param{
		operation: SUM
	}
}

################## R E L U #####################

layer {
        name: "relu55"
        type: "ReLU"
        bottom: "res55"
        top: "res55"
}



################################################

############### L A Y E R  56 ###################

################## C O N V #####################

layer{
	name: "conv56"
	type: "Convolution"
	bottom: "res55"
	top: "conv56"	
	param {
		lr_mult:1  #weights
	}
	param {
		lr_mult:2  #biases
	}
	convolution_param {
		num_output: 256 
		pad: 1
		kernel_size: 1 
		weight_filler {
			type: "xavier" 
		}
		bias_filler {
			type: "constant" 
			value: 0 
		}
	}
}

################## R E L U #####################

layer {
	name: "relu56"
	type: "ReLU"
	bottom: "conv56"
	top: "conv56"
	relu_param{
                negative_slope: 1
        }
}

########## BATCH NORMALISATION LAYER ###########

layer{
        name: "bn56"
        type: "BatchNorm"
        bottom: "conv56"
        top: "bn56"
}



################################################

############### L A Y E R  57 ###################

################## C O N V #####################

layer{
	name: "conv57"
	type: "Convolution"
	bottom: "bn56"
	top: "conv57"	
	param {
		lr_mult:1  #weights
	}
	param {
		lr_mult:2  #biases
	}
	convolution_param {
		num_output: 512 
		#pad: 1
		kernel_size: 3 
		weight_filler {
			type: "xavier" 
		}
		bias_filler {
			type: "constant" 
			value: 0 
		}
	}
}

################## R E L U #####################

layer {
	name: "relu57"
	type: "ReLU"
	bottom: "conv57"
	top: "conv57"
	relu_param{
                negative_slope: 1
        }
}

########## BATCH NORMALISATION LAYER ###########

layer{
        name: "bn57"
        type: "BatchNorm"
        bottom: "conv57"
        top: "bn57"
}


################################################

############### L A Y E R  58 ###################

############## R E S I D U A L #################

layer {
	bottom: "bn57"
	bottom: "res55"
	top: "res58"
	name: "res58"
	type: "Eltwise"
	eltwise_param{
		operation: SUM
	}
}

################## R E L U #####################

layer {
        name: "relu58"
        type: "ReLU"
        bottom: "res58"
        top: "res58"
}


################################################

############### L A Y E R  59 ###################

################## C O N V #####################

layer{
	name: "conv59"
	type: "Convolution"
	bottom: "res58"
	top: "conv59"	
	param {
		lr_mult:1  #weights
	}
	param {
		lr_mult:2  #biases
	}
	convolution_param {
		num_output: 256 
		pad: 1
		kernel_size: 1 
		weight_filler {
			type: "xavier" 
		}
		bias_filler {
			type: "constant" 
			value: 0 
		}
	}
}

################## R E L U #####################

layer {
	name: "relu59"
	type: "ReLU"
	bottom: "conv59"
	top: "conv59"
	relu_param{
                negative_slope: 1
        }
}

########## BATCH NORMALISATION LAYER ###########

layer{
        name: "bn59"
        type: "BatchNorm"
        bottom: "conv59"
        top: "bn59"
}



################################################

############### L A Y E R  60 ###################

################## C O N V #####################

layer{
	name: "conv60"
	type: "Convolution"
	bottom: "bn59"
	top: "conv60"	
	param {
		lr_mult:1  #weights
	}
	param {
		lr_mult:2  #biases
	}
	convolution_param {
		num_output: 512 
		#pad: 1
		kernel_size: 3 
		weight_filler {
			type: "xavier" 
		}
		bias_filler {
			type: "constant" 
			value: 0 
		}
	}
}

################## R E L U #####################

layer {
	name: "relu60"
	type: "ReLU"
	bottom: "conv60"
	top: "conv60"
	relu_param{
                negative_slope: 1
        }
}

########## BATCH NORMALISATION LAYER ###########

layer{
        name: "bn60"
        type: "BatchNorm"
        bottom: "conv60"
        top: "bn60"
}


################################################

############### L A Y E R  61 ###################

############## R E S I D U A L #################

layer {
	bottom: "bn60"
	bottom: "res58"
	top: "res61"
	name: "res61"
	type: "Eltwise"
	eltwise_param{
		operation: SUM
	}
}

################## R E L U #####################

layer {
        name: "relu61"
        type: "ReLU"
        bottom: "res61"
        top: "res61"
}


################################################

############### L A Y E R  62 ###################

################## C O N V #####################

layer{
	name: "conv62"
	type: "Convolution"
	bottom: "res61"
	top: "conv62"	
	param {
		lr_mult:1  #weights
	}
	param {
		lr_mult:2  #biases
	}
	convolution_param {
		num_output: 1024
		pad: 1
		kernel_size: 3
		stride: 2 
		weight_filler {
			type: "xavier" 
		}
		bias_filler {
			type: "constant" 
			value: 0 
		}
	}
}

################## R E L U #####################

layer {
	name: "relu62"
	type: "ReLU"
	bottom: "conv62"
	top: "conv62"
	relu_param{
                negative_slope: 1
        }
}

########## BATCH NORMALISATION LAYER ###########

layer{
        name: "bn62"
        type: "BatchNorm"
        bottom: "conv62"
        top: "bn62"
}


################################################

############### L A Y E R  63 ###################

################## C O N V #####################

layer{
	name: "conv63"
	type: "Convolution"
	bottom: "bn62"
	top: "conv63"	
	param {
		lr_mult:1  #weights
	}
	param {
		lr_mult:2  #biases
	}
	convolution_param {
		num_output: 512 
		pad: 1
		kernel_size: 1 
		weight_filler {
			type: "xavier" 
		}
		bias_filler {
			type: "constant" 
			value: 0 
		}
	}
}

################## R E L U #####################

layer {
	name: "relu63"
	type: "ReLU"
	bottom: "conv63"
	top: "conv63"
	relu_param{
                negative_slope: 1
        }
}


########## BATCH NORMALISATION LAYER ###########

layer{
        name: "bn63"
        type: "BatchNorm"
        bottom: "conv63"
        top: "bn63"
}


################################################

############### L A Y E R  64 ###################

################## C O N V #####################

layer{
	name: "conv64"
	type: "Convolution"
	bottom: "bn63"
	top: "conv64"	
	param {
		lr_mult:1  #weights
	}
	param {
		lr_mult:2  #biases
	}
	convolution_param {
		num_output: 1024 
		#pad: 1
		kernel_size: 3 
		weight_filler {
			type: "xavier" 
		}
		bias_filler {
			type: "constant" 
			value: 0 
		}
	}
}

################## R E L U #####################

layer {
	name: "relu64"
	type: "ReLU"
	bottom: "conv64"
	top: "conv64"
	relu_param{
                negative_slope: 1
        }
}

########## BATCH NORMALISATION LAYER ###########

layer{
        name: "bn64"
        type: "BatchNorm"
        bottom: "conv64"
        top: "bn64"
}


################################################

############### L A Y E R  65 ###################

############## R E S I D U A L #################

layer {
	bottom: "bn64"
	bottom: "bn62"
	top: "res65"
	name: "res65"
	type: "Eltwise"
	eltwise_param{
		operation: SUM
	}
}

################## R E L U #####################

layer {
        name: "relu65"
        type: "ReLU"
        bottom: "res65"
        top: "res65"
}


################################################

############### L A Y E R  66 ###################

################## C O N V #####################

layer{
	name: "conv66"
	type: "Convolution"
	bottom: "res65"
	top: "conv66"	
	param {
		lr_mult:1  #weights
	}
	param {
		lr_mult:2  #biases
	}
	convolution_param {
		num_output: 512
		pad: 1
		kernel_size: 1 
		weight_filler {
			type: "xavier" 
		}
		bias_filler {
			type: "constant" 
			value: 0 
		}
	}
}

################## R E L U #####################

layer {
	name: "relu66"
	type: "ReLU"
	bottom: "conv66"
	top: "conv66"
	relu_param{
                negative_slope: 1
        }
}


########## BATCH NORMALISATION LAYER ###########

layer{
        name: "bn66"
        type: "BatchNorm"
        bottom: "conv66"
        top: "bn66"
}


################################################

############### L A Y E R  67 ###################

################## C O N V #####################

layer{
	name: "conv67"
	type: "Convolution"
	bottom: "bn66"
	top: "conv67"	
	param {
		lr_mult:1  #weights
	}
	param {
		lr_mult:2  #biases
	}
	convolution_param {
		num_output: 1024 
		#pad: 1
		kernel_size: 3 
		weight_filler {
			type: "xavier" 
		}
		bias_filler {
			type: "constant" 
			value: 0 
		}
	}
}

################## R E L U #####################

layer {
	name: "relu67"
	type: "ReLU"
	bottom: "conv67"
	top: "conv67"
	relu_param{
                negative_slope: 1
        }
}

########## BATCH NORMALISATION LAYER ###########

layer{
        name: "bn67"
        type: "BatchNorm"
        bottom: "conv67"
        top: "bn67"
}

################################################

############### L A Y E R  68 ###################

############## R E S I D U A L #################

layer {
	bottom: "bn67"
	bottom: "res65"
	top: "res68"
	name: "res68"
	type: "Eltwise"
	eltwise_param{
		operation: SUM
	}
}

################## R E L U #####################

layer {
        name: "relu68"
        type: "ReLU"
        bottom: "res68"
        top: "res68"
}


################################################

############### L A Y E R  69 ###################

################## C O N V #####################

layer{
	name: "conv69"
	type: "Convolution"
	bottom: "res68"
	top: "conv69"	
	param {
		lr_mult:1  #weights
	}
	param {
		lr_mult:2  #biases
	}
	convolution_param {
		num_output: 512 
		pad: 1
		kernel_size: 1
		weight_filler {
			type: "xavier" 
		}
		bias_filler {
			type: "constant" 
			value: 0 
		}
	}
}

################## R E L U #####################

layer {
	name: "relu69"
	type: "ReLU"
	bottom: "conv69"
	top: "conv69"
	relu_param{
                negative_slope: 1
        }
}


########## BATCH NORMALISATION LAYER ###########

layer{
        name: "bn69"
        type: "BatchNorm"
        bottom: "conv69"
        top: "bn69"
}



################################################

############### L A Y E R  70 ###################

################## C O N V #####################

layer{
	name: "conv70"
	type: "Convolution"
	bottom: "bn69"
	top: "conv70"	
	param {
		lr_mult:1  #weights
	}
	param {
		lr_mult:2  #biases
	}
	convolution_param {
		num_output: 1024 
		#pad: 1
		kernel_size: 3 
		weight_filler {
			type: "xavier" 
		}
		bias_filler {
			type: "constant" 
			value: 0 
		}
	}
}

################## R E L U #####################

layer {
	name: "relu70"
	type: "ReLU"
	bottom: "conv70"
	top: "conv70"
	relu_param{
                negative_slope: 1
        }
}

########## BATCH NORMALISATION LAYER ###########

layer{
        name: "bn70"
        type: "BatchNorm"
        bottom: "conv70"
        top: "bn70"
}


################################################

############### L A Y E R  71 ###################

############## R E S I D U A L #################

layer {
	bottom: "bn70"
	bottom: "res68"
	top: "res71"
	name: "res71"
	type: "Eltwise"
	eltwise_param{
		operation: SUM
	}
}

################## R E L U #####################

layer {
        name: "relu71"
        type: "ReLU"
        bottom: "res71"
        top: "res71"
}



################################################

############### L A Y E R  72 ###################

################## C O N V #####################

layer{
	name: "conv72"
	type: "Convolution"
	bottom: "res71"
	top: "conv72"	
	param {
		lr_mult:1  #weights
	}
	param {
		lr_mult:2  #biases
	}
	convolution_param {
		num_output: 512 
		pad: 1
		kernel_size: 1
		weight_filler {
			type: "xavier" 
		}
		bias_filler {
			type: "constant" 
			value: 0 
		}
	}
}

################## R E L U #####################

layer {
	name: "relu72"
	type: "ReLU"
	bottom: "conv72"
	top: "conv72"
	relu_param{
                negative_slope: 1
        }
}


########## BATCH NORMALISATION LAYER ###########

layer{
        name: "bn72"
        type: "BatchNorm"
        bottom: "conv72"
        top: "bn72"
}


################################################

############### L A Y E R  73 ###################

################## C O N V #####################

layer{
	name: "conv73"
	type: "Convolution"
	bottom: "bn72"
	top: "conv73"	
	param {
		lr_mult:1  #weights
	}
	param {
		lr_mult:2  #biases
	}
	convolution_param {
		num_output: 1024 
		#pad: 1
		kernel_size: 3 
		weight_filler {
			type: "xavier" 
		}
		bias_filler {
			type: "constant" 
			value: 0 
		}
	}
}

################## R E L U #####################

layer {
	name: "relu73"
	type: "ReLU"
	bottom: "conv73"
	top: "conv73"
	relu_param{
                negative_slope: 1
        }
}

########## BATCH NORMALISATION LAYER ###########

layer{
        name: "bn73"
        type: "BatchNorm"
        bottom: "conv73"
        top: "bn73"
}


################################################

############### L A Y E R  74 ###################

############## R E S I D U A L #################

layer {
	bottom: "bn73"
	bottom: "res71"
	top: "res74"
	name: "res74"
	type: "Eltwise"
	eltwise_param{
		operation: SUM
	}
}

################## R E L U #####################

layer {
        name: "relu74"
        type: "ReLU"
        bottom: "res74"
        top: "res74"
}


####################################################
# 7 convolutional layers and then a detection layer

#75

# CONVOLUTIONAL LAYER 2 (4)
layer{
        name: "conv75"
        type: "Convolution"
        bottom: "res74"
        top: "conv75"
        param {
                lr_mult:1  #weights
        }
        param {
                lr_mult:2  #biases
        }
        convolution_param {
                num_output: 512
                #pad: 1
                kernel_size: 1
                weight_filler {
                        type: "xavier"
                }
                bias_filler {
                        type: "constant"
                        value: 0
                }
        }
}

# RELU LAYER 2 (4)

layer {
        name: "relu75"
        type: "ReLU"
        bottom: "conv75"
        top: "conv75"
        relu_param{
                negative_slope: 1
        }
}

# BATCH NORMALISATION LAYER

layer{
        name: "bn75"
        type: "BatchNorm"
        bottom: "conv75"
        top: "bn75"

}

#####################################

#76

# CONVOLUTIONAL LAYER 2 (4)
layer{
        name: "conv76"
        type: "Convolution"
        bottom: "bn75"
        top: "conv76"
        param {
                lr_mult:1  #weights
        }
        param {
                lr_mult:2  #biases
        }
        convolution_param {
                num_output: 1024
                pad: 1
                kernel_size: 3
                weight_filler {
                        type: "xavier"
                }
                bias_filler {
                        type: "constant"
                        value: 0
                }
        }
}

# RELU LAYER 2 (4)

layer {
        name: "relu76"
        type: "ReLU"
        bottom: "conv76"
        top: "conv76"
        relu_param{
                negative_slope: 1
        }
}

# BATCH NORMALISATION LAYER

layer{
        name: "bn76"
        type: "BatchNorm"
        bottom: "conv76"
        top: "bn76"

}

########################################
#77

# CONVOLUTIONAL LAYER 2 (4)
layer{
        name: "conv77"
        type: "Convolution"
        bottom: "bn76"
        top: "conv77"
        param {
                lr_mult:1  #weights
        }
        param {
                lr_mult:2  #biases
        }
        convolution_param {
                num_output: 512
                #pad: 1
                kernel_size: 1
                weight_filler {
                        type: "xavier"
                }
                bias_filler {
                        type: "constant"
                        value: 0
                }
        }
}

# RELU LAYER 2 (4)

layer {
        name: "relu77"
        type: "ReLU"
        bottom: "conv77"
        top: "conv77"
        relu_param{
                negative_slope: 1
        }
}

# BATCH NORMALISATION LAYER

layer{
        name: "bn77"
        type: "BatchNorm"
        bottom: "conv77"
        top: "bn77"

}

########################################
#78

# CONVOLUTIONAL LAYER 2 (4)
layer{
        name: "conv78"
        type: "Convolution"
        bottom: "bn77"
        top: "conv78"
        param {
                lr_mult:1  #weights
        }
        param {
                lr_mult:2  #biases
        }
        convolution_param {
                num_output: 1024
                pad: 1
                kernel_size: 3
                weight_filler {
                        type: "xavier"
                }
                bias_filler {
                        type: "constant"
                        value: 0
                }
        }
}

# RELU LAYER 2 (4)

layer {
        name: "relu78"
        type: "ReLU"
        bottom: "conv78"
        top: "conv78"
        relu_param{
                negative_slope: 1
        }
}

# BATCH NORMALISATION LAYER

layer{
        name: "bn78"
        type: "BatchNorm"
        bottom: "conv78"
        top: "bn78"

}

########################################
#79

# CONVOLUTIONAL LAYER 2 (4)
layer{
        name: "conv79"
        type: "Convolution"
        bottom: "bn78"
        top: "conv79"
        param {
                lr_mult:1  #weights
        }
        param {
                lr_mult:2  #biases
        }
        convolution_param {
                num_output: 512
                #pad: 1
                kernel_size: 1
                weight_filler {
                        type: "xavier"
                }
                bias_filler {
                        type: "constant"
                        value: 0
                }
        }
}

# RELU LAYER 2 (4)

layer {
        name: "relu79"
        type: "ReLU"
        bottom: "conv79"
        top: "conv79"
        relu_param{
                negative_slope: 1
        }
}

# BATCH NORMALISATION LAYER

layer{
        name: "bn79"
        type: "BatchNorm"
        bottom: "conv79"
        top: "bn79"

}

########################################
#80

# CONVOLUTIONAL LAYER 2 (4)
layer{
        name: "conv80"
        type: "Convolution"
        bottom: "bn79"
        top: "conv80"
        param {
                lr_mult:1  #weights
        }
        param {
                lr_mult:2  #biases
        }
        convolution_param {
                num_output: 1024
                pad: 1
                kernel_size: 3
                weight_filler {
                        type: "xavier"
                }
                bias_filler {
                        type: "constant"
                        value: 0
                }
        }
}

# RELU LAYER 2 (4)

layer {
        name: "relu80"
        type: "ReLU"
        bottom: "conv80"
        top: "conv80"
        relu_param{
                negative_slope: 1
        }
}

# BATCH NORMALISATION LAYER

layer{
        name: "bn80"
        type: "BatchNorm"
        bottom: "conv80"
        top: "bn80"

}

########################################
#81

# CONVOLUTIONAL LAYER 2 (4)
layer{
        name: "conv81"
        type: "Convolution"
        bottom: "bn80"
        top: "conv81"
        param {
                lr_mult:1  #weights
        }
        param {
                lr_mult:2  #biases
        }
        convolution_param {
                num_output: 255
                #pad: 1
                kernel_size: 1
                weight_filler {
                        type: "xavier"
                }
                bias_filler {
                        type: "constant"
                        value: 0
                }
        }
}

# RELU LAYER 2 (4)

layer {
        name: "relu81"
        type: "ReLU"
        bottom: "conv81"
        top: "conv81"
        relu_param{
                negative_slope: 1
        }
}

# BATCH NORMALISATION LAYER

layer{
        name: "bn81"
        type: "BatchNorm"
        bottom: "conv81"
        top: "bn81"

}


########################################

#82 Detection

##Detector.py
########################################

#83 route 79

########################################

#84

# CONVOLUTIONAL LAYER 2 (4)
layer{
        name: "conv84"
        type: "Convolution"
        bottom: "bn79"
        top: "conv84"
        param {
                lr_mult:1  #weights
        }
        param {
                lr_mult:2  #biases
        }
        convolution_param {
                num_output: 256
                #pad: 1
                kernel_size: 1
                weight_filler {
                        type: "xavier"
                }
                bias_filler {
                        type: "constant"
                        value: 0
                }
        }
}

# RELU LAYER 2 (4)

layer {
        name: "relu84"
        type: "ReLU"
        bottom: "conv84"
        top: "conv84"
        relu_param{
                negative_slope: 1
        }
}

# BATCH NORMALISATION LAYER

layer{
        name: "bn84"
        type: "BatchNorm"
        bottom: "conv84"
        top: "bn84"

}


########################################

#85 upsample layer


layer {
  name: "upsample85"
  type: "Deconvolution"
  bottom: "bn84"
  top: "upsample85"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: false
    kernel_size: 2
    stride: 2
  }
}


########################################

#86

# Route 85 61

layer {
  name: "concat1"
  bottom: "upsample85"
  bottom: "res61"
  top: "concat1"
  type: "Concat"
  concat_param {
    axis: 1
  }
}

########################################

#87

# CONVOLUTIONAL LAYER 2 (4)
layer{
        name: "conv87"
        type: "Convolution"
        bottom: "concat1"
        top: "conv87"
        param {
                lr_mult:1  #weights
        }
        param {
                lr_mult:2  #biases
        }
        convolution_param {
                num_output: 256
                #pad: 1
                kernel_size: 1
                weight_filler {
                        type: "xavier"
                }
                bias_filler {
                        type: "constant"
                        value: 0
                }
        }
}

# RELU LAYER 2 (4)

layer {
        name: "relu87"
        type: "ReLU"
        bottom: "conv87"
        top: "conv87"
        relu_param{
                negative_slope: 1
        }
}

# BATCH NORMALISATION LAYER

layer{
        name: "bn87"
        type: "BatchNorm"
        bottom: "conv87"
        top: "bn87"

}


########################################

#88

# CONVOLUTIONAL LAYER 2 (4)
layer{
        name: "conv88"
        type: "Convolution"
        bottom: "bn87"
        top: "conv88"
        param {
                lr_mult:1  #weights
        }
        param {
                lr_mult:2  #biases
        }
        convolution_param {
                num_output: 512
                pad: 1
                kernel_size: 3
                weight_filler {
                        type: "xavier"
                }
                bias_filler {
                        type: "constant"
                        value: 0
                }
        }
}

# RELU LAYER 2 (4)

layer {
        name: "relu88"
        type: "ReLU"
        bottom: "conv88"
        top: "conv88"
        relu_param{
                negative_slope: 1
        }
}

# BATCH NORMALISATION LAYER

layer{
        name: "bn88"
        type: "BatchNorm"
        bottom: "conv88"
        top: "bn88"

}


########################################

#89

# CONVOLUTIONAL LAYER 2 (4)
layer{
        name: "conv89"
        type: "Convolution"
        bottom: "bn88"
        top: "conv89"
        param {
                lr_mult:1  #weights
        }
        param {
                lr_mult:2  #biases
        }
        convolution_param {
                num_output: 256
                #pad: 1
                kernel_size: 1
                weight_filler {
                        type: "xavier"
                }
                bias_filler {
                        type: "constant"
                        value: 0
                }
        }
}

# RELU LAYER 2 (4)

layer {
        name: "relu89"
        type: "ReLU"
        bottom: "conv89"
        top: "conv89"
        relu_param{
                negative_slope: 1
        }
}

# BATCH NORMALISATION LAYER

layer{
        name: "bn89"
        type: "BatchNorm"
        bottom: "conv89"
        top: "bn89"

}

########################################

#90

# CONVOLUTIONAL LAYER 2 (4)
layer{
        name: "conv90"
        type: "Convolution"
        bottom: "bn89"
        top: "conv90"
        param {
                lr_mult:1  #weights
        }
        param {
                lr_mult:2  #biases
        }
        convolution_param {
                num_output: 512
                pad: 1
                kernel_size: 3
                weight_filler {
                        type: "xavier"
                }
                bias_filler {
                        type: "constant"
                        value: 0
                }
        }
}

# RELU LAYER 2 (4)

layer {
        name: "relu90"
        type: "ReLU"
        bottom: "conv90"
        top: "conv90"
        relu_param{
                negative_slope: 1
        }
}

# BATCH NORMALISATION LAYER

layer{
        name: "bn90"
        type: "BatchNorm"
        bottom: "conv90"
        top: "bn90"

}
########################################

#91

# CONVOLUTIONAL LAYER 2 (4)
layer{
        name: "conv91"
        type: "Convolution"
        bottom: "bn90"
        top: "conv91"
        param {
                lr_mult:1  #weights
        }
        param {
                lr_mult:2  #biases
        }
        convolution_param {
                num_output: 256
                #pad: 1
                kernel_size: 1
                weight_filler {
                        type: "xavier"
                }
                bias_filler {
                        type: "constant"
                        value: 0
                }
        }
}

# RELU LAYER 2 (4)

layer {
        name: "relu91"
        type: "ReLU"
        bottom: "conv91"
        top: "conv91"
        relu_param{
                negative_slope: 1
        }
}

# BATCH NORMALISATION LAYER

layer{
        name: "bn91"
        type: "BatchNorm"
        bottom: "conv91"
        top: "bn91"

}

########################################

#92

# CONVOLUTIONAL LAYER 2 (4)
layer{
        name: "conv92"
        type: "Convolution"
        bottom: "bn91"
        top: "conv92"
        param {
                lr_mult:1  #weights
        }
        param {
                lr_mult:2  #biases
        }
        convolution_param {
                num_output: 512
                pad: 1
                kernel_size: 3
                weight_filler {
                        type: "xavier"
                }
                bias_filler {
                        type: "constant"
                        value: 0
                }
        }
}

# RELU LAYER 2 (4)

layer {
        name: "relu92"
        type: "ReLU"
        bottom: "conv92"
        top: "conv92"
        relu_param{
                negative_slope: 1
        }
}

# BATCH NORMALISATION LAYER

layer{
        name: "bn92"
        type: "BatchNorm"
        bottom: "conv92"
        top: "bn92"

}
########################################

#93

# CONVOLUTIONAL LAYER 2 (4)
layer{
        name: "conv93"
        type: "Convolution"
        bottom: "bn92"
        top: "conv93"
        param {
                lr_mult:1  #weights
        }
        param {
                lr_mult:2  #biases
        }
        convolution_param {
                num_output: 255
                #pad: 1
                kernel_size: 1
                weight_filler {
                        type: "xavier"
                }
                bias_filler {
                        type: "constant"
                        value: 0
                }
        }
}

# RELU LAYER 2 (4)

layer {
        name: "relu93"
        type: "ReLU"
        bottom: "conv93"
        top: "conv93"
        relu_param{
                negative_slope: 1
        }
}

# BATCH NORMALISATION LAYER

layer{
        name: "bn93"
        type: "BatchNorm"
        bottom: "conv93"
        top: "bn93"

}

########################################

#94 DETECTION LAYER

########################################

#95 Route 91

########################################

#96
# CONVOLUTIONAL LAYER 2 (4)
layer{
        name: "conv96"
        type: "Convolution"
        bottom: "bn91"
        top: "conv96"
        param {
                lr_mult:1  #weights
        }
        param {
                lr_mult:2  #biases
        }
        convolution_param {
                num_output: 128
                #pad: 1
                kernel_size: 1
                weight_filler {
                        type: "xavier"
                }
                bias_filler {
                        type: "constant"
                        value: 0
                }
        }
}

# RELU LAYER 2 (4)

layer {
        name: "relu96"
        type: "ReLU"
        bottom: "conv96"
        top: "conv96"
        relu_param{
                negative_slope: 1
        }
}

# BATCH NORMALISATION LAYER

layer{
        name: "bn96"
        type: "BatchNorm"
        bottom: "conv96"
        top: "bn96"

}


########################################

#97 upsample layer


layer {
  name: "upsample97"
  type: "Deconvolution"
  bottom: "bn96"
  top: "upsample97"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: false
    kernel_size: 2
    stride: 2
  }
}


########################################

#98

# Route 97 36

layer {
  name: "concat2"
  bottom: "upsample97"
  bottom: "res36"
  top: "concat2"
  type: "Concat"
  concat_param {
    axis: 1
  }
}

########################################

#99

# CONVOLUTIONAL LAYER 2 (4)
layer{
        name: "conv99"
        type: "Convolution"
        bottom: "concat2"
        top: "conv99"
        param {
                lr_mult:1  #weights
        }
        param {
                lr_mult:2  #biases
        }
        convolution_param {
                num_output: 128
                pad: 1
                kernel_size: 1
                weight_filler {
                        type: "xavier"
                }
                bias_filler {
                        type: "constant"
                        value: 0
                }
        }
}

# RELU LAYER 2 (4)

layer {
        name: "relu99"
        type: "ReLU"
        bottom: "conv99"
        top: "conv99"
        relu_param{
                negative_slope: 1
        }
}

# BATCH NORMALISATION LAYER

layer{
        name: "bn99"
        type: "BatchNorm"
        bottom: "conv99"
        top: "bn99"

}


########################################

#100

# CONVOLUTIONAL LAYER 2 (4)
layer{
        name: "conv100"
        type: "Convolution"
        bottom: "bn99"
        top: "conv100"
        param {
                lr_mult:1  #weights
        }
        param {
                lr_mult:2  #biases
        }
        convolution_param {
                num_output: 256
                pad: 1
                kernel_size: 3
                weight_filler {
                        type: "xavier"
                }
                bias_filler {
                        type: "constant"
                        value: 0
                }
        }
}

# RELU LAYER 2 (4)

layer {
        name: "relu100"
        type: "ReLU"
        bottom: "conv100"
        top: "conv100"
        relu_param{
                negative_slope: 1
        }
}

# BATCH NORMALISATION LAYER

layer{
        name: "bn100"
        type: "BatchNorm"
        bottom: "conv100"
        top: "bn100"

}

########################################

#101

# CONVOLUTIONAL LAYER 2 (4)
layer{
        name: "conv101"
        type: "Convolution"
        bottom: "bn100"
        top: "conv101"
        param {
                lr_mult:1  #weights
        }
        param {
                lr_mult:2  #biases
        }
        convolution_param {
                num_output: 128
                pad: 1
                kernel_size: 1
                weight_filler {
                        type: "xavier"
                }
                bias_filler {
                        type: "constant"
                        value: 0
                }
        }
}

# RELU LAYER 2 (4)

layer {
        name: "relu101"
        type: "ReLU"
        bottom: "conv101"
        top: "conv101"
        relu_param{
                negative_slope: 1
        }
}

# BATCH NORMALISATION LAYER

layer{
        name: "bn101"
        type: "BatchNorm"
        bottom: "conv101"
        top: "bn101"

}

########################################

#102

# CONVOLUTIONAL LAYER 2 (4)
layer{
        name: "conv102"
        type: "Convolution"
        bottom: "bn101"
        top: "conv102"
        param {
                lr_mult:1  #weights
        }
        param {
                lr_mult:2  #biases
        }
        convolution_param {
                num_output: 256
                pad: 1
                kernel_size: 3
                weight_filler {
                        type: "xavier"
                }
                bias_filler {
                        type: "constant"
                        value: 0
                }
        }
}

# RELU LAYER 2 (4)

layer {
        name: "relu102"
        type: "ReLU"
        bottom: "conv102"
        top: "conv102"
        relu_param{
                negative_slope: 1
        }
}

# BATCH NORMALISATION LAYER

layer{
        name: "bn102"
        type: "BatchNorm"
        bottom: "conv102"
        top: "bn102"

}
########################################

#103

# CONVOLUTIONAL LAYER 2 (4)
layer{
        name: "conv103"
        type: "Convolution"
        bottom: "bn102"
        top: "conv103"
        param {
                lr_mult:1  #weights
        }
        param {
                lr_mult:2  #biases
        }
        convolution_param {
                num_output: 128
                pad: 1
                kernel_size: 1
                weight_filler {
                        type: "xavier"
                }
                bias_filler {
                        type: "constant"
                        value: 0
                }
        }
}

# RELU LAYER 2 (4)

layer {
        name: "relu103"
        type: "ReLU"
        bottom: "conv103"
        top: "conv103"
        relu_param{
                negative_slope: 1
        }
}

# BATCH NORMALISATION LAYER

layer{
        name: "bn103"
        type: "BatchNorm"
        bottom: "conv103"
        top: "bn103"

}
########################################

#104

# CONVOLUTIONAL LAYER 2 (4)
layer{
        name: "conv104"
        type: "Convolution"
        bottom: "bn103"
        top: "conv104"
        param {
                lr_mult:1  #weights
        }
        param {
                lr_mult:2  #biases
        }
        convolution_param {
                num_output: 256
                pad: 1
                kernel_size: 3
                weight_filler {
                        type: "xavier"
                }
                bias_filler {
                        type: "constant"
                        value: 0
                }
        }
}

# RELU LAYER 2 (4)

layer {
        name: "relu104"
        type: "ReLU"
        bottom: "conv104"
        top: "conv104"
        relu_param{
                negative_slope: 1
        }
}

# BATCH NORMALISATION LAYER

layer{
        name: "bn104"
        type: "BatchNorm"
        bottom: "conv104"
        top: "bn104"

}


########################################

#105

# CONVOLUTIONAL LAYER 2 (4)
layer{
        name: "conv105"
        type: "Convolution"
        bottom: "bn104"
        top: "conv105"
        param {
                lr_mult:1  #weights
        }
        param {
                lr_mult:2  #biases
        }
        convolution_param {
                num_output: 255
                pad: 1
                kernel_size: 1
                weight_filler {
                        type: "xavier"
                }
                bias_filler {
                        type: "constant"
                        value: 0
                }
        }
}

# RELU LAYER 2 (4)

layer {
        name: "relu105"
        type: "ReLU"
        bottom: "conv105"
        top: "conv105"
        relu_param{
                negative_slope: 1
        }
}

# BATCH NORMALISATION LAYER

layer{
        name: "bn105"
        type: "BatchNorm"
        bottom: "conv105"
        top: "bn105"

}


##################################################

#106

#DETECTION




























#####################################################
# POOLING LAYER

layer {
	name:"avgpool"
	type: "Pooling"
	bottom: "bn105"
	top: "avgpool"
	pooling_param {	
		pool: AVE
		##kernel_size: 2 #2x2 kernel
		#####stride: 2
		global_pooling: true
	}
}

####################################################


# FULLY CONNECTED LAYER

layer {
	bottom: "avgpool"
	top: "fc1000"
	name: "fc1000"
	type: "InnerProduct"
	param {
		lr_mult: 1
	}
	param {
		lr_mult: 2
	}
	inner_product_param {
		num_output: 1000
		weight_filler {
			type: "xavier"
		}
		bias_filler {
			type: "constant"
		}
	}
}

####################################################

####################################################


# FULLY CONNECTED LAYER

layer {
	bottom: "fc1000"
	top: "fc80"
	name: "fc80"
	type: "InnerProduct"
	param {
		lr_mult: 1
	}
	param {
		lr_mult: 2
	}
	inner_product_param {
		num_output: 80
		weight_filler {
			type: "xavier"
		}
		bias_filler {
			type: "constant"
		}
	}
}

####################################################

####################################################


######
### SOFTMAX LAYER
#


### Softmax without loss for testing

layer {
	bottom: "fc80"
	top: "prob"
	name: "prob"
	type: "Softmax"
	include {
		phase: TEST
	}
}

##########################################################
